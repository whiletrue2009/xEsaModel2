{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f8a4b33",
   "metadata": {},
   "source": [
    "# xEsaModel GPU 训练 (Google Colab)\n",
    "\n",
    "## 环境准备\n",
    "\n",
    "1. 检查 GPU 类型\n",
    "2. 安装依赖\n",
    "3. 克隆代码仓库\n",
    "4. 准备数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4940c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查 GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7053d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装依赖\n",
    "!pip install torch==2.1.0+cu118 -f https://download.pytorch.org/whl/cu118/torch_stable.html\n",
    "!pip install transformers==4.31.0 tokenizers==0.13.3 accelerate==0.21.0\n",
    "!pip install peft==0.6.0 bitsandbytes==0.41.1 flash-attn==2.3.3\n",
    "!pip install datasets==3.3.2 llmtuner==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1677cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 克隆代码仓库\n",
    "!git clone https://github.com/whiletrue2009/xEsaModel2.git\n",
    "%cd xEsaModel2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fa38a5",
   "metadata": {},
   "source": [
    "## 数据准备\n",
    "\n",
    "有两种方式准备数据：\n",
    "1. 从 Google Drive 挂载\n",
    "2. 直接上传到 Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b819e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方式1：挂载 Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 复制数据集\n",
    "!cp -r /content/drive/MyDrive/xEsaModel2/data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8443ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方式2：直接上传\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# 解压数据集\n",
    "!mkdir -p data/train\n",
    "!mv train_augmented.json data/train/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0967ab",
   "metadata": {},
   "source": [
    "## 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d3aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置环境变量\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "# 运行训练\n",
    "!python -c \"from llmtuner.train import run_exp; run_exp()\" \\\n",
    "    --model_name_or_path \"deepseek-ai/deepseek-coder-1.3b-base\" \\\n",
    "    --output_dir \"outputs/distill_gpu\" \\\n",
    "    --dataset_dir \"data\" \\\n",
    "    --dataset \"train/train_augmented.json\" \\\n",
    "    --template \"alpaca\" \\\n",
    "    --stage \"sft\" \\\n",
    "    --finetuning_type \"lora\" \\\n",
    "    --lora_rank 32 \\\n",
    "    --lora_alpha 128 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --lora_target \"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\" \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --gradient_accumulation_steps 2 \\\n",
    "    --cutoff_len 2048 \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --lr_scheduler_type \"cosine_with_restarts\" \\\n",
    "    --warmup_steps 100 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --num_train_epochs 8 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 50 \\\n",
    "    --max_grad_norm 0.5 \\\n",
    "    --do_train \\\n",
    "    --overwrite_cache \\\n",
    "    --report_to \"none\" \\\n",
    "    --log_level \"debug\" \\\n",
    "    --fp16 \\\n",
    "    --gradient_checkpointing \\\n",
    "    --load_in_8bit \\\n",
    "    --torch_compile \\\n",
    "    --optim \"adamw_torch\" \\\n",
    "    --max_memory \"14GB\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3c253f",
   "metadata": {},
   "source": [
    "## 保存模型\n",
    "\n",
    "训练完成后，我们有两种方式保存模型：\n",
    "1. 保存到 Google Drive\n",
    "2. 直接下载到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb9b690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方式1：保存到 Google Drive\n",
    "!cp -r outputs/distill_gpu /content/drive/MyDrive/xEsaModel2/outputs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2d505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方式2：打包下载\n",
    "!tar -czf model.tar.gz outputs/distill_gpu/\n",
    "from google.colab import files\n",
    "files.download('model.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1997f2",
   "metadata": {},
   "source": [
    "## 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a558ca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "def load_model():\n",
    "    # 加载基础模型\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"deepseek-ai/deepseek-coder-1.3b-base\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        load_in_8bit=True,\n",
    "        max_memory={\"0\": \"14GB\"}\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"deepseek-ai/deepseek-coder-1.3b-base\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    model = PeftModel.from_pretrained(base_model, \"outputs/distill_gpu\")\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_response(model, tokenizer, prompt):\n",
    "    prompt_template = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{prompt}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt_template, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=512,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            use_cache=True\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "# 加载模型并测试\n",
    "model, tokenizer = load_model()\n",
    "\n",
    "test_questions = [\n",
    "    \"什么是服务区资金归集管理办法？\",\n",
    "    \"资金归集的定义是什么？\",\n",
    "    \"服务区资金归集管理办法适用于哪些单位？\",\n",
    "    \"江苏交控营运事业部的主要职责是什么？\",\n",
    "    \"资金归集流程中，签订聚合支付协议涉及哪些内容？\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n问题: {question}\")\n",
    "    response = generate_response(model, tokenizer, question)\n",
    "    print(f\"回答: {response}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
