model_name_or_path: "deepseek-ai/deepseek-coder-1.3b-base"
teacher_model_name_or_path: "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

model_type: "AutoModelForCausalLM"
trust_remote_code: true
use_auth_token: false

dataset:
  type: "text"
  format: "alpaca"
  file_name: "data/train/train_augmented.json"

data_processing:
  num_workers: 4
  batch_size: 8
  max_seq_length: 2048
  preprocessing_num_workers: 4

training:
  num_train_epochs: 8
  learning_rate: 5e-5
  lr_scheduler_type: "cosine_with_restarts"
  warmup_steps: 100
  weight_decay: 0.01
  max_grad_norm: 0.5
  gradient_accumulation_steps: 4
  logging_steps: 5
  save_steps: 50
  eval_steps: 100
  save_total_limit: 2
  output_dir: "outputs/distill_gpu"
  overwrite_output_dir: true
  ddp_find_unused_parameters: false
  gradient_checkpointing: true
  torch_compile: true
  fp16: true
  bf16: false

distillation:
  temperature: 2.0
  alpha_ce: 0.5
  alpha_kl: 0.5
  alpha_hidden: 0.0
  alpha_cos: 0.0
  kl_loss_type: "kl"
  hidden_loss_type: "mse"
  hidden_mapping_type: "linear"
  hidden_mapping_layers: []

lora:
  use_lora: true
  r: 32
  alpha: 128
  dropout: 0.05
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

quantization:
  load_in_8bit: false
  load_in_4bit: false

device_map: "auto"
use_cuda: true 