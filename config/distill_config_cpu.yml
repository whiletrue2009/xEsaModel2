model_name_or_path: "deepseek-ai/deepseek-coder-1.3b-base"
teacher_model_name_or_path: "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

model_type: "AutoModelForCausalLM"
trust_remote_code: true
use_auth_token: false

dataset:
  type: "text"
  format: "alpaca"
  file_name: "data/train/train.json"

data_processing:
  num_workers: 1  # CPU 模式下减少工作进程
  batch_size: 1   # 减小批次大小
  max_seq_length: 512  # 进一步减小序列长度
  preprocessing_num_workers: 1

training:
  num_train_epochs: 3
  learning_rate: 2e-4
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03
  weight_decay: 0.01
  max_grad_norm: 1.0
  gradient_accumulation_steps: 16  # 增加梯度累积
  logging_steps: 10
  save_steps: 100
  eval_steps: 100
  save_total_limit: 1
  output_dir: "outputs/distill_cpu"
  overwrite_output_dir: true
  ddp_find_unused_parameters: false
  gradient_checkpointing: true
  torch_compile: false
  fp16: false
  bf16: false

distillation:
  temperature: 2.0
  alpha_ce: 0.5
  alpha_kl: 0.5
  alpha_hidden: 0.0
  alpha_cos: 0.0
  kl_loss_type: "kl"
  hidden_loss_type: "mse"
  hidden_mapping_type: "linear"
  hidden_mapping_layers: []

lora:
  use_lora: true  # 启用 LoRA
  r: 8
  alpha: 32
  dropout: 0.1
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]

quantization:
  load_in_8bit: false
  load_in_4bit: true  # 启用 4-bit 量化
  compute_dtype: "float32"
  double_quant: true

# CPU 特定设置
device_map: "cpu"
use_cpu: true
no_cuda: true 